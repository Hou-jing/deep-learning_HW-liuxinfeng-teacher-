{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hou-jing/deep-learning_HW-liuxinfeng-teacher-/blob/main/%E8%87%AA%E5%8A%A8%E5%86%99%E8%AF%97.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/jgxnrzfhziczmv0/tang.zip?dl=0  -O train.zip\n",
        "!unzip -q train.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mvcm7opN2od",
        "outputId": "dc891fdb-b687-4160-8750-37a8d3a7d21f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-31 13:45:05--  https://www.dropbox.com/s/jgxnrzfhziczmv0/tang.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/jgxnrzfhziczmv0/tang.zip [following]\n",
            "--2022-03-31 13:45:06--  https://www.dropbox.com/s/raw/jgxnrzfhziczmv0/tang.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc0a03215fe5bb10d17e187e4b0c.dl.dropboxusercontent.com/cd/0/inline/Bih7kPtfSIPwpXVglWxlWD9J27a9wKtByGAozOEkapegfcjI6V2r7SBe9Bb_Zr7tqbJZuk0Nxdm80xzgDYzyy5ipOLMuaj9yfstuSWg-RBGdqAbH2pGJKpqs2APjFn4EoE3rNJaZ3gLZIn-8TYPaF_9l2YgyYG-NzXwIehmcW1DSTA/file# [following]\n",
            "--2022-03-31 13:45:06--  https://uc0a03215fe5bb10d17e187e4b0c.dl.dropboxusercontent.com/cd/0/inline/Bih7kPtfSIPwpXVglWxlWD9J27a9wKtByGAozOEkapegfcjI6V2r7SBe9Bb_Zr7tqbJZuk0Nxdm80xzgDYzyy5ipOLMuaj9yfstuSWg-RBGdqAbH2pGJKpqs2APjFn4EoE3rNJaZ3gLZIn-8TYPaF_9l2YgyYG-NzXwIehmcW1DSTA/file\n",
            "Resolving uc0a03215fe5bb10d17e187e4b0c.dl.dropboxusercontent.com (uc0a03215fe5bb10d17e187e4b0c.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc0a03215fe5bb10d17e187e4b0c.dl.dropboxusercontent.com (uc0a03215fe5bb10d17e187e4b0c.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BigwOvbb44xoz1dCcozFRzVkAWB9tbeBl56e90xICqQKalBjD6RRBbB_LBUAfujgqOpW3hidOKasdKMCXKh1RXngVGbFD5hJYEe-MFUszh5W82U7kHck0EyHRtNqeuCxw4ZiBHgT2C9NABX-MMJ__9kUbkV9xMwZfh2bgWJzlkXnIxLOY2ZElPsJHDdB0untxVU6grpjCphEAbb12HpE8DN7UbqYxdd8TjmAfTmi4CjrQywJGEX1Wu-Nmi3oS7Ic5r789-R75kxIKdgoMlkMiPgYfeKy9-QZAKZ_OHC0vLPb1tBZX8SNZWIG3CdS0Jb4ECGamolJlCFn1yrOy5yG4YwtU6VnX1uli9gE1vaDJClUMGRa2IwIcEXvojsAHThS4Yed1mI-_nuZTdar-xMT7Z7tV5yKlefnKNmr1Zv4umdDKA/file [following]\n",
            "--2022-03-31 13:45:06--  https://uc0a03215fe5bb10d17e187e4b0c.dl.dropboxusercontent.com/cd/0/inline2/BigwOvbb44xoz1dCcozFRzVkAWB9tbeBl56e90xICqQKalBjD6RRBbB_LBUAfujgqOpW3hidOKasdKMCXKh1RXngVGbFD5hJYEe-MFUszh5W82U7kHck0EyHRtNqeuCxw4ZiBHgT2C9NABX-MMJ__9kUbkV9xMwZfh2bgWJzlkXnIxLOY2ZElPsJHDdB0untxVU6grpjCphEAbb12HpE8DN7UbqYxdd8TjmAfTmi4CjrQywJGEX1Wu-Nmi3oS7Ic5r789-R75kxIKdgoMlkMiPgYfeKy9-QZAKZ_OHC0vLPb1tBZX8SNZWIG3CdS0Jb4ECGamolJlCFn1yrOy5yG4YwtU6VnX1uli9gE1vaDJClUMGRa2IwIcEXvojsAHThS4Yed1mI-_nuZTdar-xMT7Z7tV5yKlefnKNmr1Zv4umdDKA/file\n",
            "Reusing existing connection to uc0a03215fe5bb10d17e187e4b0c.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5719611 (5.5M) [application/zip]\n",
            "Saving to: ‘train.zip’\n",
            "\n",
            "train.zip           100%[===================>]   5.45M  19.7MB/s    in 0.3s    \n",
            "\n",
            "2022-03-31 13:45:07 (19.7 MB/s) - ‘train.zip’ saved [5719611/5719611]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a98yKFuxN1lA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import *\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 4 \n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 读取数据"
      ],
      "metadata": {
        "id": "t75jB7n0VdPX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j6OpFJxpN1lE"
      },
      "outputs": [],
      "source": [
        "# 读入预处理的数据\n",
        "datas = np.load(\"./tang.npz\",allow_pickle=True)\n",
        "data = datas['data']\n",
        "ix2word = datas['ix2word'].item()\n",
        "word2ix = datas['word2ix'].item()\n",
        "    \n",
        "# 转为torch.Tensor\n",
        "data = torch.from_numpy(data)\n",
        "train_loader = DataLoader(data, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BgoGYBYSN1lE"
      },
      "outputs": [],
      "source": [
        "class PoetryModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(PoetryModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=3)\n",
        "        self.classifier=nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim, 512), \n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.Linear(512, 2048), \n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(2048, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, input, hidden = None):\n",
        "        seq_len, batch_size = input.size()\n",
        "        \n",
        "        if hidden is None:\n",
        "            h_0 = input.data.new(3, batch_size, self.hidden_dim).fill_(0).float()\n",
        "            c_0 = input.data.new(3, batch_size, self.hidden_dim).fill_(0).float()\n",
        "        else:\n",
        "            h_0, c_0 = hidden\n",
        "\n",
        "        embeds = self.embedding(input)\n",
        "        output, hidden = self.lstm(embeds, (h_0, c_0))\n",
        "        output = self.classifier(output.view(seq_len * batch_size, -1))\n",
        "        \n",
        "        return output, hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3_q4-kgIN1lF"
      },
      "outputs": [],
      "source": [
        "# 配置模型，是否继续上一次的训练\n",
        "model = PoetryModel(len(word2ix),embedding_dim = 128,hidden_dim = 256)\n",
        "\n",
        "model_path = ''         # 预训练模型路径\n",
        "if model_path:\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "model.to(DEVICE)\n",
        "    \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-3)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = StepLR(optimizer, step_size=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnpXk5duN1lG"
      },
      "source": [
        "# 模型训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sKN6OENuN1lH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model, dataloader, ix2word, word2ix, device, optimizer, scheduler, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    for batch_idx, data in enumerate(dataloader):\n",
        "        data = data.long().transpose(1, 0).contiguous()\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        input, target = data[:-1, :], data[1:, :]\n",
        "        output, _ = model(input)\n",
        "        loss = criterion(output, target.view(-1))\n",
        "        loss.backward()  \n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "            \n",
        "        if (batch_idx+1) % 200 == 0:\n",
        "            print('train epoch: {} [{}/{} ({:.0f}%)]\\tloss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data[1]), len(dataloader.dataset),\n",
        "                100. * batch_idx / len(dataloader), loss.item()))\n",
        "            \n",
        "    train_loss *= BATCH_SIZE\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    print('\\ntrain epoch: {}\\t average loss: {:.6f}\\n'.format(epoch,train_loss))\n",
        "    scheduler.step()\n",
        "    \n",
        "    return train_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIYbKF5LN1lI",
        "outputId": "603396d9-6f7d-42fd-e394-a0ebb60d1d48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train epoch: 1 [3184/57580 (6%)]\tloss: 2.911373\n",
            "train epoch: 1 [6384/57580 (11%)]\tloss: 2.695083\n",
            "train epoch: 1 [9584/57580 (17%)]\tloss: 2.268628\n",
            "train epoch: 1 [12784/57580 (22%)]\tloss: 2.361314\n",
            "train epoch: 1 [15984/57580 (28%)]\tloss: 2.074336\n",
            "train epoch: 1 [19184/57580 (33%)]\tloss: 2.125256\n",
            "train epoch: 1 [22384/57580 (39%)]\tloss: 2.271052\n",
            "train epoch: 1 [25584/57580 (44%)]\tloss: 2.261040\n",
            "train epoch: 1 [28784/57580 (50%)]\tloss: 2.839618\n",
            "train epoch: 1 [31984/57580 (56%)]\tloss: 2.924308\n",
            "train epoch: 1 [35184/57580 (61%)]\tloss: 2.588761\n",
            "train epoch: 1 [38384/57580 (67%)]\tloss: 2.475260\n",
            "train epoch: 1 [41584/57580 (72%)]\tloss: 1.970198\n",
            "train epoch: 1 [44784/57580 (78%)]\tloss: 2.375037\n",
            "train epoch: 1 [47984/57580 (83%)]\tloss: 2.417519\n",
            "train epoch: 1 [51184/57580 (89%)]\tloss: 2.181257\n",
            "train epoch: 1 [54384/57580 (94%)]\tloss: 1.943453\n",
            "\n",
            "train epoch: 1\t average loss: 2.457768\n",
            "\n",
            "train epoch: 2 [3184/57580 (6%)]\tloss: 2.187993\n",
            "train epoch: 2 [6384/57580 (11%)]\tloss: 1.964591\n",
            "train epoch: 2 [9584/57580 (17%)]\tloss: 2.563280\n",
            "train epoch: 2 [12784/57580 (22%)]\tloss: 2.297023\n",
            "train epoch: 2 [15984/57580 (28%)]\tloss: 2.231971\n",
            "train epoch: 2 [19184/57580 (33%)]\tloss: 2.190139\n",
            "train epoch: 2 [22384/57580 (39%)]\tloss: 1.765331\n",
            "train epoch: 2 [25584/57580 (44%)]\tloss: 1.961411\n",
            "train epoch: 2 [28784/57580 (50%)]\tloss: 2.151248\n",
            "train epoch: 2 [31984/57580 (56%)]\tloss: 1.682462\n",
            "train epoch: 2 [35184/57580 (61%)]\tloss: 2.592742\n",
            "train epoch: 2 [38384/57580 (67%)]\tloss: 2.671372\n",
            "train epoch: 2 [41584/57580 (72%)]\tloss: 2.142159\n",
            "train epoch: 2 [44784/57580 (78%)]\tloss: 2.180276\n",
            "train epoch: 2 [47984/57580 (83%)]\tloss: 2.102253\n",
            "train epoch: 2 [51184/57580 (89%)]\tloss: 2.316011\n",
            "train epoch: 2 [54384/57580 (94%)]\tloss: 2.095365\n",
            "\n",
            "train epoch: 2\t average loss: 2.201900\n",
            "\n",
            "train epoch: 3 [3184/57580 (6%)]\tloss: 1.899331\n",
            "train epoch: 3 [6384/57580 (11%)]\tloss: 2.298231\n",
            "train epoch: 3 [9584/57580 (17%)]\tloss: 2.406267\n",
            "train epoch: 3 [12784/57580 (22%)]\tloss: 2.205478\n",
            "train epoch: 3 [15984/57580 (28%)]\tloss: 1.710218\n",
            "train epoch: 3 [19184/57580 (33%)]\tloss: 2.249003\n",
            "train epoch: 3 [22384/57580 (39%)]\tloss: 2.229245\n",
            "train epoch: 3 [25584/57580 (44%)]\tloss: 1.889638\n",
            "train epoch: 3 [28784/57580 (50%)]\tloss: 2.251246\n",
            "train epoch: 3 [31984/57580 (56%)]\tloss: 1.675259\n",
            "train epoch: 3 [35184/57580 (61%)]\tloss: 2.291666\n",
            "train epoch: 3 [38384/57580 (67%)]\tloss: 1.691754\n",
            "train epoch: 3 [41584/57580 (72%)]\tloss: 2.295034\n",
            "train epoch: 3 [44784/57580 (78%)]\tloss: 2.206975\n",
            "train epoch: 3 [47984/57580 (83%)]\tloss: 1.999100\n",
            "train epoch: 3 [51184/57580 (89%)]\tloss: 2.127583\n",
            "train epoch: 3 [54384/57580 (94%)]\tloss: 2.530004\n",
            "\n",
            "train epoch: 3\t average loss: 2.110011\n",
            "\n",
            "train epoch: 4 [3184/57580 (6%)]\tloss: 2.376987\n",
            "train epoch: 4 [6384/57580 (11%)]\tloss: 2.118603\n",
            "train epoch: 4 [9584/57580 (17%)]\tloss: 2.075374\n",
            "train epoch: 4 [12784/57580 (22%)]\tloss: 1.427251\n",
            "train epoch: 4 [15984/57580 (28%)]\tloss: 2.322614\n",
            "train epoch: 4 [19184/57580 (33%)]\tloss: 1.980317\n",
            "train epoch: 4 [22384/57580 (39%)]\tloss: 1.889516\n",
            "train epoch: 4 [25584/57580 (44%)]\tloss: 2.336269\n",
            "train epoch: 4 [28784/57580 (50%)]\tloss: 2.290143\n",
            "train epoch: 4 [31984/57580 (56%)]\tloss: 2.228712\n",
            "train epoch: 4 [35184/57580 (61%)]\tloss: 1.803329\n",
            "train epoch: 4 [38384/57580 (67%)]\tloss: 1.954587\n",
            "train epoch: 4 [41584/57580 (72%)]\tloss: 1.684548\n",
            "train epoch: 4 [44784/57580 (78%)]\tloss: 2.085620\n",
            "train epoch: 4 [47984/57580 (83%)]\tloss: 1.693686\n",
            "train epoch: 4 [51184/57580 (89%)]\tloss: 1.838475\n",
            "train epoch: 4 [54384/57580 (94%)]\tloss: 2.189187\n",
            "\n",
            "train epoch: 4\t average loss: 2.054238\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_losses = []\n",
        "\n",
        "for epoch in range(1,EPOCHS+1):\n",
        "    tr_loss = train(model,train_loader,ix2word,word2ix,DEVICE,optimizer,scheduler,epoch)\n",
        "    train_losses.append(tr_loss)\n",
        "    \n",
        "# 保存模型\n",
        "filename = \"model\" + str(time.time()) + \".pth\"\n",
        "torch.save(model.state_dict(), filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N24ApQeqN1lJ"
      },
      "source": [
        "# 模型评估"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, start_words, ix2word, word2ix, max_gen_len, prefix_words=None):\n",
        "    # 读取唐诗的第一句\n",
        "    results = list(start_words)\n",
        "    start_word_len = len(start_words)\n",
        "\n",
        "    # 设置第一个词为<START>\n",
        "    input = torch.Tensor([word2ix['<START>']]).view(1, 1).long()\n",
        "    input = input.to(DEVICE)\n",
        "    hidden = None\n",
        "\n",
        "    if prefix_words:\n",
        "        for word in prefix_words:\n",
        "            output, hidden = model(input, hidden)\n",
        "            input = Variable(input.data.new([word2ix[word]])).view(1, 1)\n",
        "\n",
        "    # 生成唐诗\n",
        "    for i in range(max_gen_len):\n",
        "        output, hidden = model(input, hidden)\n",
        "        # 读取第一句\n",
        "        if i < start_word_len:\n",
        "            w = results[i]\n",
        "            input = input.data.new([word2ix[w]]).view(1, 1)\n",
        "        # 生成后面的句子\n",
        "        else:\n",
        "            top_index = output.data[0].topk(1)[1][0].item()\n",
        "            w = ix2word[top_index]\n",
        "            results.append(w)\n",
        "            input = input.data.new([top_index]).view(1, 1)\n",
        "        # 结束标志\n",
        "        if w == '<EOP>':\n",
        "            del results[-1]\n",
        "            break\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "86LuqjiqRqrW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmbWgleIN1lK",
        "outputId": "82e39274-50ae-46ab-e47b-93f1c7ee4a2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "百日依山尽，千年出处同。\n",
            "山川无定事，山色不能生。\n",
            "白发无人在，青云有酒行。\n",
            "一生无所有，三十四千重。\n",
            "白发无人在，青云有故乡。\n",
            "\n"
          ]
        }
      ],
      "source": [
        "start_words = '百日依山尽'  # 唐诗的第一句\n",
        "max_gen_len = 60        # 生成唐诗的最长长度\n",
        "\n",
        "prefix_words = None\n",
        "results = generate(model, start_words, ix2word, word2ix, max_gen_len, prefix_words)\n",
        "poetry = ''\n",
        "for word in results:\n",
        "    poetry += word\n",
        "    if word == '。' or word == '!':\n",
        "        poetry += '\\n'\n",
        "        \n",
        "print(poetry)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvW27hIFN1lK"
      },
      "source": [
        "# 生成藏头诗"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bxbibUlmN1lL"
      },
      "outputs": [],
      "source": [
        "def gen_acrostic(model, start_words, ix2word, word2ix, prefix_words=None):\n",
        "    # 读取唐诗的“头”\n",
        "    results = []\n",
        "    start_word_len = len(start_words)\n",
        "    \n",
        "    # 设置第一个词为<START>\n",
        "    input = (torch.Tensor([word2ix['<START>']]).view(1, 1).long())\n",
        "    input = input.to(DEVICE)\n",
        "    hidden = None\n",
        "\n",
        "    index = 0            # 指示已生成了多少句\n",
        "    pre_word = '<START>' # 上一个词\n",
        "    \n",
        "    if prefix_words:\n",
        "        for word in prefix_words:\n",
        "            output, hidden = model(input, hidden)\n",
        "            input = Variable(input.data.new([word2ix[word]])).view(1, 1)\n",
        "\n",
        "    # 生成藏头诗\n",
        "    for i in range(max_gen_len_acrostic):\n",
        "        output, hidden = model(input, hidden)\n",
        "        top_index = output.data[0].topk(1)[1][0].item()\n",
        "        w = ix2word[top_index]\n",
        "\n",
        "        # 如果遇到标志一句的结尾，喂入下一个“头”\n",
        "        if (pre_word in {u'。', u'！', '<START>'}):\n",
        "            # 如果生成的诗已经包含全部“头”，则结束\n",
        "            if index == start_word_len:\n",
        "                break\n",
        "            # 把“头”作为输入喂入模型\n",
        "            else:\n",
        "                w = start_words[index]\n",
        "                index += 1\n",
        "                input = (input.data.new([word2ix[w]])).view(1, 1)\n",
        "                \n",
        "        # 否则，把上一次预测作为下一个词输入\n",
        "        else:\n",
        "            input = (input.data.new([word2ix[w]])).view(1, 1)\n",
        "        results.append(w)\n",
        "        pre_word = w\n",
        "        \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUCW8nHoN1lL",
        "outputId": "c3c1a57a-1e1f-4473-fc09-98da9a63eaf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "风雨不可见，山川无所思。\n",
            "和风吹白日，落日出青山。\n",
            "月照青山上，风吹白日中。\n",
            "明月照秋草，清秋入秋风。\n",
            "\n"
          ]
        }
      ],
      "source": [
        "start_words_acrostic = '风和月明'  # 唐诗的“头”\n",
        "max_gen_len_acrostic = 120               # 生成唐诗的最长长度\n",
        "prefix_words = None\n",
        "results_acrostic = gen_acrostic(model, start_words_acrostic, ix2word, word2ix,prefix_words)\n",
        "\n",
        "poetry = ''\n",
        "for word in results_acrostic:\n",
        "    poetry += word\n",
        "    if word == '。' or word == '!':\n",
        "        poetry += '\\n'\n",
        "        \n",
        "print(poetry)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "自动写诗.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}